\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[left=1cm,right=1cm,top=2cm,bottom=2cm]{geometry}

\lstset{ 
language=C++,
stepnumber=2,
}

%opening
\title{Марковский генератор текстов $n$-ого порядка}
\author{Поликарпов Андрей}

\begin{document}

\maketitle

\newpage
\section{Постановка задачи}
Требуется реализовать марковский генератор текста n-ого порядка.
Логически состоит из двух исполняемых файлов: обучающего и генерирующего на основе обученной модели.
\begin{enumerate}
\item Обучающий -
	получает на вход текстовый файл в ASCII кодировке, содержащий текст на естественном языке. Пунктуация не важна, 
	но желательно привести текст к единому регистру, чтобы увеличить заполняемость цепи. Также задается параметр N - порядок цепи.
	По входному тексту строится марковская цепь, результат сериализуется в файл на жестком диске.
\item  Генерирующий -
получает на вход аналогичный предыдущему текстовый файл, содержащий начальный отрывок из n слов
и число K - количество слов, которые надо достроить к отрывку на основании созданной предыдущим исполняемым файлом марковской цепи.
Полученный текст нужно выводить на поток стандартного вывода. 
\end{enumerate}


\section{Как решена задача}

\subsection{Построение цепи}
Марковская цепь хранится как ассоциативный массив (std::map), в котором ключом является список слов\\ std::list<std::string>, а значением еще один ассоциативный массив (std::map<std::string, int>).
Размер ключа  - порядок марковской цепи $n$.\\
Построение происходит таким образом:\\
Исходный  список слов пустой. Для него ассоциируется следующий массив: <первое слово> $\rightarrow$ 1.
Далее, список слов снова пустой, кроме последнего элемента. Этот элемент равен первому слову. Такому ключу сопоставляется массив: <второе слово> $\rightarrow$ 1. Таким образом заполняются первые $n$ элементов ассоциативного массива. Далее, когда в  списке слов уже нет пустых элементов, ключом являются первые $n$ слов исходного текста.
Идет поиск такого списка слов в тексте, в качестве значения в ассоциативном массиве будут слова, которые идут следующими после вхождения такого списка в исходном тексте, вместе с количеством вхождений.
Например:\\
\indent Это утверждение. Это следующее утверждение. Это еще одно утверждение.\\
Тогда этот текст будет представлен в таком виде:\\
$[$ `` '',  `` ''\ $]$ $\rightarrow$ $\{$ "это" $= 1$ $\}$\\
$[$ `` '', "это"$]$ $\rightarrow$ $\{$ "утверждение" $= 1$ $\}$\\
$[$ "это"\ , "утверждение" $]$ $\rightarrow$ $\{$ "это" $= 1$ $\}$\\
$[$ "утверждение"\ ,  "это" $]$ $\rightarrow$ $\{$ "следующее" $= 1$, "eще" $=1$ $\}$\\
...
и так далее.\\
Исходный текст приводится к одному регистру, все знаки препинания и цифры удаляются.
\subsection{Генерация текста}
Есть список слов $T$ который надо продолжить. Также есть построенная марковская цепь $n$-ого порядка. Для генерации следующего слова происходит поиск в такой таблице по следующему ключу:\\
Ключом является список из последних $n$ слов из списка $T$ в порядке следования. Следующее слово будет выбрано как значение по этому ключу в марковской цепи с учетом частоты. Например:\\
Пусть для ключа ["this"\ , "is"] значением в цепи является \{"ololo" $= 1$, "the" $= 2$\}. При обращении по такому ключу в двух случаях из трех вернется "the". Пусть возвращено слово "the". Далее, ключ обновится и станет ["is"\ , "the"]. Процесс продолжается. \\
В данной реализации, если возвращаемое слово пустое, то процесс будет продолжен, но уже с полностью пустым ключом, для которого точно есть значение в цепи в виде первого слова с частотой 1. Процесс закончится пока не будет построено $K$ слов.

\section{Сериализация}
Для сериализации/десериализации используется библиотека Protocol Buffers.\\
.proto-файл приложен к проекту.


\section{Присутствующие недостатки}
\begin{enumerate}
	\item Способ удаления знаков препинания подразумевает наличие после знака препинания пробела. В реальности это не всегда так.
	\item Данные из файла в итоге хранятся в std::vector, возможно, это не очень хорошее решение(с точки зрения производительности).
	\item Алгоритм поиска совпадений двух списков (std::list) в функции Fit() не оптимален.
	 \footnote {в приложенном файле input.txt 6207 слов, порядок цепи равен 3, обучение занимает примерно 16 секунд. Если порядок цепи 2 - примерно 11 секунд.}
\end{enumerate}

\section{Что можно улучшить}
\begin{enumerate}
	\item Добавить тесты для генератора + добавить Doxygen-документацию.
	\item Протестировать производительность + исправить (возможно, хранить .
		все слова не в std::list<std::string>, а просто в std::string c разделителем).
	\item Возможно, стоит использовать FlatBuffers, а не Protocol Buffers.
	\item Сделать configure или сделать на cmake.
	\item Сделать полноценный класс для исключений.
	\item Добавить возможность генерировать отдельные буквы, а не слова.

\end{enumerate}
\section{Дополнительная информация}

Для работы программы необходимо установить Google Protocol Buffers.\\
Используется С++11.\\
В файле run примеры запуска программы.\\
В файле input.txt содержатся субтитры к фильму "Брат-2" на английском языке.

\newpage

\begin{thebibliography}{3}
	\bibitem{Wiki} \href{https://en.wikipedia.org/wiki/Markov_chain}{Markov Chain}
	\bibitem{Coursera} \href{https://class.coursera.org/nlp/lecture/preview}{Coursera Text Mining}
	\bibitem{protobuf-git} \href{https://github.com/google/protobuf}{ProtoBuf GitHub}
\end{thebibliography}
\end{document}
